}
TransitionsData=merge(Transitions, structure[,c('part_index','part_id')],by.x = 'x', by.y='part_index',all.x = TRUE)
TransitionsData=TransitionsData[,c('part_id','y','provenance','destination')]
names(TransitionsData)[1]='x'
TransitionsData=merge(TransitionsData, structure[,c('part_index','part_id')],by.x = 'y', by.y='part_index',all.x = TRUE)
TransitionsData=TransitionsData[,c('x','part_id','provenance','destination')]
names(TransitionsData)[2]='y'
Destinations_stats = data.frame(part_index=part_indexes,destination_next=0.0,destination_past=0.0,
destination_future=0.0, destination_not_linear=0.0)
for(aPart in 1:max(structure$part_index))
{
partDestinations=Transitions[Transitions$x==aPart,c('y','destination')]
Destinations_stats[Destinations_stats$part_index==aPart,]$destination_past =
sum(partDestinations[partDestinations$y<aPart,]$destination)
Destinations_stats[Destinations_stats$part_index==aPart,]$destination_future =
sum(partDestinations[partDestinations$y>aPart+1,]$destination)
if(aPart<max(structure$part_index))
Destinations_stats[Destinations_stats$part_index==aPart,]$destination_next =
partDestinations[partDestinations$y==aPart+1,]$destination
Destinations_stats[Destinations_stats$part_index==aPart,]$destination_not_linear =
1.0 - (Destinations_stats[Destinations_stats$part_index==aPart,]$destination_next)
}
Destinations_stats = rbind(c(0,mean(Destinations_stats$destination_next),mean(Destinations_stats$destination_past),
mean(Destinations_stats$destination_future),mean(Destinations_stats$destination_not_linear)),
Destinations_stats)
Provenances_stats = data.frame(part_index=part_indexes,provenance_prev=0.0,provenance_past=0.0,
provenance_future=0.0, provenance_not_linear=0.0)
for(aPart in 1:max(structure$part_index))
{
partProvenances=Transitions[Transitions$x==aPart,c('y','provenance')]
Provenances_stats[Provenances_stats$part_index==aPart,]$provenance_past =
sum(partProvenances[partProvenances$y<aPart-1,]$provenance)
Provenances_stats[Provenances_stats$part_index==aPart,]$provenance_future =
sum(partProvenances[partProvenances$y>aPart,]$provenance)
if(aPart>1)
Provenances_stats[Provenances_stats$part_index==aPart,]$provenance_prev =
partProvenances[partProvenances$y==aPart-1,]$provenance
Provenances_stats[Provenances_stats$part_index==aPart,]$provenance_not_linear =
1.0 - (Provenances_stats[Provenances_stats$part_index==aPart,]$provenance_prev)
}
Provenances_stats = rbind(c(0,mean(Provenances_stats$provenance_prev),mean(Provenances_stats$provenance_past),
mean(Provenances_stats$provenance_future),mean(Provenances_stats$provenance_not_linear)),
Provenances_stats)
#################Interest####################
users=unique(data$user_id)
nusers=length(users)
PartData = merge(PartData, Reads[,-c(1)], all.x = TRUE)
PartData = merge(PartData, Interest[,c('part_id','RS_nb')], all.x = TRUE)
rs_nb = Interest[which(Interest$part_id== structure[which(structure$type=='course'),]$part_id),]$RS_nb
PartData$Actions_tx = PartData$Actions_nb / nrow(data)
PartData$readers_tx = PartData$Readers / nusers
PartData$rs_tx = PartData$RS_nb / rs_nb
PartData$invspeed = 0
if(min(PartData$speed>0)){
minspeed = min(PartData[PartData$speed>0,]$speed)
maxspeed = max(PartData[PartData$speed>0,]$speed)
PartData$invspeed = -1
PartData[PartData$speed>0,]$invspeed = maxspeed- PartData[PartData$speed>0,]$speed
PartData[PartData$speed>0,]$invspeed =range01(PartData[PartData$speed>0,]$invspeed, na.rm=TRUE)
}
PartData$interest =PartData$Actions_tx + PartData$readers_tx + PartData$rs_tx
#+ PartData$invspeed
PartData[PartData$interest>0,]$interest = range01(PartData[PartData$interest>0,]$interest, na.rm=TRUE)
#################FIN####################
PartData = merge(PartData, Provenances_stats,  by = 'part_index',all.x = TRUE)
PartData = merge(PartData, Destinations_stats,  by = 'part_index',all.x = TRUE)
PartData$reading_not_linear = PartData$provenance_not_linear+ PartData$destination_not_linear
PartData$reading_not_linear = range01(PartData$reading_not_linear, na.rm=TRUE)
PartData = merge(PartData, Ruptures[,-c(1)], all.x = TRUE)
PartData$recovery=PartData$direct_recovery+PartData$distant_next_recovery+PartData$distant_prev_recovery+PartData$next_recovery+PartData$prev_recovery
PartData$norecovery=PartData$rupture-PartData$recovery
allRup = max(PartData$rupture, na.rm=TRUE)
finalRupt = max(PartData$norecovery, na.rm=TRUE)
PartData$rupture_tx = PartData$rupture/allRup
PartData$norecovery_tx = PartData$norecovery/finalRupt
PartData$resume_future= (PartData$distant_next_recovery)/PartData$recovery
PartData$resume_past= (PartData$prev_recovery+PartData$distant_prev_recovery)/PartData$recovery
PartData$resume_abnormal_tx = PartData$resume_future + PartData$resume_past
################### RELECTURE ######################################
PartData$rereads_tx = 0
PartData$rereads_globratio = 0
allRereads = sum(PartData[which(PartData$type=='chapitre'),]$Rereadings, na.rm=TRUE)
PartData[which(PartData$type=='chapitre'),]$rereads_tx =  PartData[which(PartData$type=='chapitre'),]$Rereadings / PartData[which(PartData$type=='chapitre'),]$Readings
PartData[which(PartData$type=='chapitre'),]$rereads_globratio =  PartData[which(PartData$type=='chapitre'),]$Rereadings / allRereads
# for chapters
tomeIds = PartData[which(PartData$type=='partie'),]$part_id
for(i in 1:length(tomeIds)){
PartData[which(PartData$part_id==tomeIds[i]),]$rereads_tx =  mean(PartData[which(PartData$parent_id==tomeIds[i]),]$rereads_tx)
PartData[which(PartData$part_id==tomeIds[i]),]$rereads_globratio =  mean(PartData[which(PartData$parent_id==tomeIds[i]),]$rereads_globratio)
}
PartData$mean.tx_total_rereaders = round(100 * PartData$Rereaders / PartData$Readers, 2)
PartData$mean.tx_total_readers = round(100 * PartData$Rereaders / nusers, 2)
PartData$rereads_seq_tx = 0
sumseq = sum(PartData[which(PartData$type=='chapitre'),]$Sequential_rereadings, na.rm=TRUE)
PartData[which(PartData$type=='chapitre'),]$rereads_seq_tx =   PartData[which(PartData$type=='chapitre'),]$Sequential_rereadings / sumseq
# for tomes
tomeIds = PartData[which(PartData$type=='partie'),]$part_id
for(i in 1:length(tomeIds)){
PartData[which(PartData$part_id==tomeIds[i]),]$rereads_seq_tx =  mean(PartData[which(PartData$parent_id==tomeIds[i]),]$rereads_seq_tx)
}
# for all course
PartData[which(PartData$type=='course'),]$rereads_seq_tx =  mean(PartData[which(PartData$type=='chapitre'),]$rereads_seq_tx)
PartData$rereads_dec_tx = 0
sumdec = sum(PartData[which(PartData$type=='chapitre'),]$Decaled_rereadings, na.rm=TRUE)
PartData[which(PartData$type=='chapitre'),]$rereads_dec_tx =   PartData[which(PartData$type=='chapitre'),]$Decaled_rereadings / sumdec
# for chapters
tomeIds = PartData[which(PartData$type=='partie'),]$part_id
for(i in 1:length(tomeIds)){
PartData[which(PartData$part_id==tomeIds[i]),]$rereads_dec_tx =  mean(PartData[which(PartData$parent_id==tomeIds[i]),]$rereads_dec_tx)
}
# for all course
PartData[which(PartData$type=='course'),]$rereads_dec_tx =  mean(PartData[which(PartData$type=='chapitre'),]$rereads_dec_tx)
####################FIN####################
PartData[PartData$type=='course',]$speed =
mean(PartData[PartData$type=='chapitre',]$speed)
PartData[PartData$type=='course',]$Actions_tx =
mean(PartData[PartData$type=='chapitre',]$Actions_tx)
PartData[PartData$type=='course',]$readers_tx =
mean(PartData[PartData$type=='chapitre',]$readers_tx)
PartData[PartData$type=='course',]$rs_tx =
mean(PartData[PartData$type=='chapitre',]$rs_tx)
PartData[PartData$type=='course',]$rupture_tx =
mean(PartData[PartData$type=='chapitre',]$rupture_tx)
PartData[PartData$type=='course',]$norecovery_tx =
mean(PartData[PartData$type=='chapitre',]$norecovery_tx)
PartData[PartData$type=='course',]$rereads_tx =
mean(PartData[PartData$type=='chapitre',]$rereads_tx)
PartData=PartData[,c("part_index","part_id","parent_id","title","type", "slug", "max.duration"  ,  "mean.duration"  ,
"median.duration" ,"q1.duration" , "q3.duration", "size", "speed" ,"interest" , "Actions_nb",
"Readers", "Rereaders" , "Readings","readers_tx", "Actions_tx","rs_tx",
"Rereadings", "rereads_tx", "rereads_seq_tx" ,  "rereads_dec_tx",
"Sequential_rereadings" ,"Decaled_rereadings",  "reading_not_linear",
"provenance_prev" , "provenance_not_linear", "provenance_past",  "provenance_future" ,
"destination_next", "destination_not_linear","destination_past" ,"destination_future"  ,
"rupture_tx", "norecovery_tx","resume_abnormal_tx" ,"resume_past","resume_future")]
colnames(PartData)[1]="id"
####################Arrange structure
#st = PartData[PartData$id>0,c('id','parent_id')]
#st=unique(st[order(st$id),]$parent_id)
#st = data.frame(part_id=st, tome_index=1:length(st))
#PartData=merge(PartData,st,all.x = TRUE)
#write.csv2(structure,'structure.csv')
#MANUALLY !!!!!!!!!!!!!!!!!!!
#structure = read.csv('structure.csv', stringsAsFactors=FALSE)
#save(structure, file="structure.rdata")
#View(PartData[,c('tome_index','chap_index','part_index')])
#save(PartData, file='../coursesdata/PartData.rdata')
#################### EXPORT
########## Stats
CourseData = data.frame(id=0, title=PartData[PartData$type=='course','title'])
CourseData$nactions =sum(PartData[PartData$type=='chapitre','Actions_nb'])
CourseData$nusers =length(unique(data$user_id))
CourseData$nRS = nrow(RS)
CourseData$RS_meanparts = mean(RS$nparts)
CourseData$RS_meanperuser = nrow(RS)/nusers
CourseData$ob_begin=as.character(min(data$date))
CourseData$ob_end=as.character(max(data$date))
res = list(PartData=PartData,CourseData=CourseData, TransitionsData=TransitionsData)
return(res)
}
###############  COURSE ISSUES
course_issues_calculation <- function(data, structure,PartData){
######################INDICATORS INIT.###############################
minInterest =    # interest
minVisits =   	# Actions_tx
minReaders = 		# readers_tx 			<----------
minRS = 		# rs_tx  			<----------
maxSpeed = 		# speed
maxRereadings = 	# rereads_tx
maxDijRereadings = 	# rereads_dec_tx
maxConjRereadings = 	# rereads_seq_tx
readingLinearity = 	# reading_not_linear  		<----------
provenanceLinearity = 	# provenance_not_linear
provenanceFuture = 	# provenance_future
provenancePast = 	# provenance_past
destinationLinearity = # destination_not_linear
destinationFuture = 	# destination_future
destinationPast = 	# destination_past
maxRSStops = 		# rupture_tx
maxFinalStops = 	# norecovery_tx
resumeLinearity = 	# resume_abnormal_tx  		<----------
recoveryPast = 	# resume_past
recoveryFuture =	#resume_future
data.frame(part_id=integer(),value=character(),classe=character(),issueCode=character(),delta=numeric(),error_value=numeric())
chaptersData = PartData[which(PartData$type=='chapitre'),]
####### TROP PEU D'INTERET
byChaps = chaptersData[(DoubleMADsFromMedian(chaptersData$interest)>2)&(chaptersData$interest<median(chaptersData$interest) ),c('part_id','interest')]
if(nrow(byChaps)>0){
byChaps$classe="interest"
byChaps$issueCode="min"
byChaps$delta  = median(chaptersData$interest,na.rm = TRUE)- byChaps$interest
byChaps$error_value = round(median(chaptersData$interest,na.rm = TRUE)/ byChaps$interest,2)
minInterest = byChaps
}
####### NOMBRE DE VISITES TROP PEU
byChaps = chaptersData[(DoubleMADsFromMedian(chaptersData$Actions_tx)>2)&(chaptersData$Actions_tx<median(chaptersData$Actions_tx) ),c('part_id','Actions_tx')]
if(nrow(byChaps)>0){
byChaps$classe="Actions_tx"
byChaps$issueCode="min"
byChaps$delta  = median(chaptersData$Actions_tx,na.rm = TRUE)- byChaps$Actions_tx
byChaps$error_value = round(median(chaptersData$Actions_tx,na.rm = TRUE)/ byChaps$Actions_tx,2)
minVisits = byChaps
}
####### NOMBRE DE LECTEURS TROP PEU
byChaps = chaptersData[(DoubleMADsFromMedian(chaptersData$readers_tx)>2)&(chaptersData$readers_tx<median(chaptersData$readers_tx) ),c('part_id','readers_tx')]
if(nrow(byChaps)>0){
byChaps$classe="readers_tx"
byChaps$issueCode="min"
byChaps$delta  = median(chaptersData$readers_tx,na.rm = TRUE)- byChaps$readers_tx
byChaps$error_value = round(median(chaptersData$readers_tx,na.rm = TRUE)/ byChaps$readers_tx,2)
minReaders = byChaps
}
####### NOMBRE DE RS TROP PEU
byChaps = chaptersData[(DoubleMADsFromMedian(chaptersData$rs_tx)>2)&(chaptersData$rs_tx<median(chaptersData$rs_tx) ),c('part_id','rs_tx')]
if(nrow(byChaps)>0){
byChaps$classe="rs_tx"
byChaps$issueCode="min"
byChaps$delta  = median(chaptersData$rs_tx,na.rm = TRUE)- byChaps$rs_tx
byChaps$error_value = round(median(chaptersData$rs_tx,na.rm = TRUE)/ byChaps$rs_tx,2)
minRS = byChaps
}
################## Vitesse MAX
if(min(PartData$speed>0)){
byChaps = chaptersData[(DoubleMADsFromMedian(chaptersData$speed)>2)&(chaptersData$speed>median(chaptersData$speed) ),c('part_id','speed')]
if(nrow(byChaps)>0){
byChaps$classe="speed"
byChaps$issueCode="max"
byChaps$delta = byChaps$speed - median(chaptersData$speed,na.rm = TRUE)
byChaps$error_value = round(byChaps$speed / median(chaptersData$speed,na.rm = TRUE) ,2)
minSpeed =  byChaps
}
}
################## Vitesse MIN
if(min(PartData$speed>0)){
byChaps = chaptersData[(DoubleMADsFromMedian(chaptersData$speed)>2)&(chaptersData$speed<median(chaptersData$speed) ),c('part_id','speed')]
if(nrow(byChaps)>0){
byChaps$classe="speed"
byChaps$issueCode="min"
byChaps$delta = median(chaptersData$speed,na.rm = TRUE) /byChaps$speed
byChaps$error_value = round(median(chaptersData$speed,na.rm = TRUE) /byChaps$speed  ,2)
maxSpeed =  byChaps
}
}
####### MAX REREADINGS
byChaps = chaptersData[(DoubleMADsFromMedian(chaptersData$rereads_tx)>2)&
(chaptersData$rereads_tx>median(chaptersData$rereads_tx) ),c('part_id','rereads_tx')]
if(nrow(byChaps)>0){
byChaps$classe="rereads_tx"
byChaps$issueCode="max"
byChaps$delta=byChaps$rereads_tx-median(chaptersData$rereads_tx)
byChaps$error_value = round(byChaps$rereads_tx/median(chaptersData$rereads_tx),2)
maxRereadings =  byChaps
}
####### Max Conjoint Rereadings
byChaps = chaptersData[(DoubleMADsFromMedian(chaptersData$rereads_seq_tx)>2)&
(chaptersData$rereads_seq_tx>median(chaptersData$rereads_seq_tx) ),c('part_id','rereads_seq_tx')]
if(nrow(byChaps)>0){
byChaps=byChaps[,c('part_id','rereads_seq_tx')]
byChaps$classe="rereads_seq_tx"
byChaps$issueCode="max"
byChaps$delta=byChaps$rereads_seq_tx-median(chaptersData$rereads_seq_tx)
byChaps$error_value = round(byChaps$rereads_seq_tx/median(chaptersData$rereads_seq_tx),2)
maxConjRereadings =  byChaps
}
####### Max Disjoint Rereadings
byChaps = chaptersData[(DoubleMADsFromMedian(chaptersData$rereads_dec_tx)>2)&
(chaptersData$rereads_dec_tx>median(chaptersData$rereads_dec_tx) ),c('part_id','rereads_dec_tx')]
if(nrow(byChaps)>0){
byChaps=byChaps[,c('part_id','rereads_dec_tx')]
byChaps$classe="rereads_dec_tx"
byChaps$issueCode="max"
byChaps$delta=byChaps$rereads_dec_tx-median(chaptersData$rereads_dec_tx)
byChaps$error_value = round(byChaps$rereads_dec_tx/median(chaptersData$rereads_dec_tx),2)
maxDijRereadings =  byChaps
}
####### Reading Linearity
byChaps = chaptersData[(DoubleMADsFromMedian(chaptersData$reading_not_linear)>2)&
(chaptersData$reading_not_linear>median(chaptersData$reading_not_linear) ),c('part_id','reading_not_linear')]
if(nrow(byChaps)>0){
byChaps$classe="reading_not_linear"
byChaps$issueCode="max"
byChaps$delta = byChaps$reading_not_linear
byChaps$error_value =round(100*byChaps$reading_not_linear,2)
readingLinearity = byChaps
}
####### Provenance Linearity
byChaps=subset(chaptersData, chaptersData$provenance_not_linear>0.5 , select=c('part_id','provenance_not_linear'))
if(nrow(byChaps)>0){
byChaps$classe="provenance_not_linear"
byChaps$issueCode="max"
byChaps$error_value=round(100*byChaps$provenance_not_linear,2)
byChaps$delta = byChaps$provenance_not_linear
provenanceLinearity = byChaps
}
####### Provenance Future
byChaps=subset(chaptersData, (chaptersData$provenance_future>0.5) && (chaptersData$id!=1) , select=c('part_id','provenance_future'))
if(nrow(byChaps)>0){
byChaps$classe="provenance_future"
byChaps$issueCode="max"
byChaps$error_value=round(100*byChaps$provenance_future,2)
byChaps$delta = byChaps$provenance_future
provenanceFuture = byChaps
}
####### Provenance Past
byChaps=subset(chaptersData, (chaptersData$provenance_past>0.3)&& (chaptersData$id!=1) , select=c('part_id','provenance_past'))
if(nrow(byChaps)>0){
byChaps$classe="provenance_past"
byChaps$issueCode="TransProvShift"
byChaps$error_value=round(100*byChaps$provenance_past,2)
byChaps$delta = byChaps$provenance_past
provenancePast = byChaps
}
####### Destination Linearity
byChaps=subset(chaptersData, chaptersData$destination_not_linear >0.5 , select=c('part_id','destination_not_linear'))
if(nrow(byChaps)>0){
byChaps$classe="destination_not_linear"
byChaps$issueCode="TransDestShift"
byChaps$error_value=round(100*byChaps$destination_not_linear,2)
byChaps$delta=byChaps$destination_not_linear
destinationLinearity = byChaps
}
####### Destination Future
byChaps=subset(chaptersData, chaptersData$destination_future >0.3 , select=c('part_id','destination_future'))
if(nrow(byChaps)>0){
byChaps$classe="destination_future"
byChaps$issueCode="max"
byChaps$error_value=round(100*byChaps$destination_future,2)
byChaps$delta=byChaps$destination_future
destinationFuture = byChaps
}
####### Destination Past
byChaps=subset(chaptersData, chaptersData$destination_past >0.5 , select=c('part_id','destination_past'))
if(nrow(byChaps)>0){
byChaps$classe="destination_past"
byChaps$issueCode="max"
byChaps$error_value=round(100*byChaps$destination_past,2)
byChaps$delta=byChaps$destination_past
destinationPast = byChaps
}
########### Session end
byChaps = chaptersData[(DoubleMADsFromMedian(chaptersData$rupture_tx)>2)&
(chaptersData$rupture_tx>median(chaptersData$rupture_tx) ),c('part_id','rupture_tx')]
if(nrow(byChaps)>0){
byChaps$classe="rupture_tx"
byChaps$issueCode="max"
byChaps$error_value=round(100*byChaps$rupture_tx,2)
byChaps$delta=round(100*byChaps$rupture_tx,2)
maxRSStops =   byChaps
}
####### Max Stops
byChaps = chaptersData[(DoubleMADsFromMedian(chaptersData$norecovery_tx)>2)&
(chaptersData$norecovery_tx>median(chaptersData$norecovery_tx) ),c('part_id','norecovery_tx')]
if(nrow(byChaps)>0){
byChaps$classe="norecovery_tx"
byChaps$issueCode="max"
byChaps$delta=byChaps$norecovery_tx - median(chaptersData$norecovery_tx)
byChaps$error_value = round(100*byChaps$norecovery_tx,2)
maxFinalStops =   byChaps
}
####### Recovery ANormal
byChaps = subset(chaptersData, chaptersData$resume_abnormal_tx > 0.5 , select=c('part_id','resume_abnormal_tx'))
if(nrow(byChaps)>0){
byChaps$classe="resume_abnormal_tx"
byChaps$issueCode="max"
byChaps$error_value=round(100*byChaps$resume_abnormal_tx,2)
byChaps$delta = byChaps$resume_abnormal_tx;
resumeLinearity =   byChaps
}
####### Recovery Past
byChaps = subset(chaptersData, chaptersData$resume_past > 0.33 , select=c('part_id','resume_past'))
if(nrow(byChaps)>0){
byChaps$classe="resume_past"
byChaps$issueCode="max"
byChaps$error_value=round(100*byChaps$resume_past,2)
byChaps$delta = byChaps$resume_past
recoveryPast =   byChaps
}
####### Recovery Future
byChaps = subset(chaptersData, chaptersData$resume_future > 0.33 , select=c('part_id','resume_future'))
if(nrow(byChaps)>0){
byChaps$classe="resume_future"
byChaps$issueCode="max"
byChaps$delta = byChaps$resume_future
byChaps$error_value=round(100*byChaps$resume_future,2)
recoveryFuture =   byChaps
}
##############CONCATENATE EVERYTHING##############
names(minInterest)[c(1,2)]=
names(minVisits)[c(1,2)]=
names(minReaders)[c(1,2)]=
names(maxRereadings)[c(1,2)]=
names(maxDijRereadings)[c(1,2)]=
names(maxConjRereadings)[c(1,2)]=
names(readingLinearity)[c(1,2)]=
names(provenanceLinearity)[c(1,2)]=
names(provenancePast)[c(1,2)]=
names(provenanceFuture)[c(1,2)]=
names(destinationLinearity)[c(1,2)]=
names(destinationPast)[c(1,2)]=
names(destinationFuture)[c(1,2)]=
names(maxRSStops)[c(1,2)]=
names(maxFinalStops)[c(1,2)]=
names(resumeLinearity)[c(1,2)]=
names(recoveryPast)[c(1,2)]=
names(recoveryFuture)[c(1,2)]=
c("part_id","value")
facts =
rbind(
minInterest,
minVisits,
minReaders,
maxRereadings,
maxDijRereadings,
maxConjRereadings,
readingLinearity,
provenanceLinearity,
provenancePast,
provenanceFuture,
destinationLinearity,
destinationPast,
destinationFuture,
maxRSStops,
maxFinalStops,
resumeLinearity,
recoveryFuture,
recoveryPast)
rownames(facts)=NULL
if(min(PartData$speed>0)){
names(minSpeed)[c(1,2)]=
names(maxSpeed)[c(1,2)]=c("part_id","value")
facts =  rbind(facts,minSpeed,maxSpeed)
}
return(facts)
}
for(i in 3: length(allF)){
print(paste('COURSE :',i-1,length(allF)-1,sep=' / '))
do_course(paste(allF[i],'data.csv',sep='/'),paste(allF[i],'structure.json',sep='/'),paste(allF[i],'size.structure.csv',sep='/'))
}
i
allF[i]
csv_f = "/home/madjid/Dropbox/rcoreada/Dataset/3449001/data.csv"
json_f = "/home/madjid/Dropbox/rcoreada/Dataset/3449001/structure.json"
size_f = "/home/madjid/Dropbox/rcoreada/Dataset/3449001/size.structure.csv"
coursesdata_home
setwd(paste(coursesdata_home,'3449001',sep='/')
)
load('data.rdata')
load('structure.rdata')
sz = read.csv(size_f)
sz=sz[,c("part_id","size"    ,   "nb_img"     ,"vid_length")]
names(sz)
colnames(sz)
sz = read.csv2(size_f)
colnames(sz)
sz=sz[,c("part_id","size"    ,   "nb_img"     ,"vid_length")]
getwd()
sz = read.csv2(size_f)
colnames(sz)
size_f
sz = read.csv2(size_f)
sz=sz[,c("part_id","size"    ,   "nb_img"     ,"vid_length")]
drops <-c("size"    ,   "nb_img"     ,"vid_length")
sz = read.csv2(size_f)
sz=sz[,c("part_id","size"    ,   "nb_img"     ,"vid_length")]
sz = read.csv2(size_f)
names(sz)
sz = read.csv(size_f)
sz=sz[,c("part_id","size"    ,   "nb_img"     ,"vid_length")]
drops <-c("size"    ,   "nb_img"     ,"vid_length")
structure = structure[ , !(names(structure) %in% drops)]
structure=merge(structure,sz,by='part_id', all.x = TRUE)
getwd()
save(structure, file='structure.rdata')
indicators = indicators_calculation(data,structure)
data = indicators$data
structure = indicators$structure
RS = indicators$RS
Interest = indicators$Interest
Reads = indicators$Reads
Ruptures = indicators$Ruptures
partFollow = indicators$partFollow
save(RS, file="RS.rdata")
save(Interest, file="Interest.rdata")
save(Reads, file="Reads.rdata")
save(Ruptures,file="Ruptures.rdata")
save(partFollow, file="partFollow.rdata")
dir.create(file.path(coreaDataURL,courseId), showWarnings = FALSE)
courseDataURL=paste(coreaDataURL,courseId,sep='/')
CourseDataCalc = course_data_calculation(data,structure, indicators)
CourseData=CourseDataCalc$CourseData
PartData = CourseDataCalc$PartData
TransitionsData =  CourseDataCalc$TransitionsData
save(PartData,file='PartData.rdata')
meltParts=melt(PartData, id.vars = 'id')
meltedCourseStats = melt(CourseData,  id.vars ="id")
meltedCourseData = rbind(meltParts,meltedCourseStats)
if(nrow(meltedCourseData[is.nan(meltedCourseData$value),])>0) meltedCourseData[is.nan(meltedCourseData$value),]$value=0
if(nrow(meltedCourseData[is.na(meltedCourseData$value),])>0) meltedCourseData[is.na(meltedCourseData$value),]$value=0
facts = course_issues_calculation(data, structure,PartData)
save(facts, file="facts.rdata")
CourseData.json = toJSON(meltedCourseData)
cat(CourseData.json, file=paste(courseDataURL,"data.json",sep='/'))
TransitionsData.json = toJSON(TransitionsData)
cat(TransitionsData.json, file=paste(courseDataURL,"navigation.json",sep='/'))
facts.json = toJSON(facts)
cat(facts.json, file=paste(courseDataURL,"facts.json",sep='/'))
print('COREADA OK!')
i
i=10
length(allF)
print(paste('COURSE :',i-1,length(allF)-1,sep=' / '))
do_course(paste(allF[i],'data.csv',sep='/'),paste(allF[i],'structure.json',sep='/'),paste(allF[i],'size.structure.csv',sep='/'))
